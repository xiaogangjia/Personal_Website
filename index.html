<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiaogang Jia</title>
  
  <meta name="author" content="Xiaogang Jia">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    /* Apply a circular mask to the image */
    .circular-image {
      width: 100px; /* Adjust the width and height as needed */
      height: 100px;
      border-radius: 50%;
      overflow: hidden;
    }

    /* Ensure the image fills the circular container */
    .circular-image img {
      width: 100%;
      height: auto;
      display: block;
    }
  </style>
  <title>Circular Image in Table</title>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiaogang Jia</name>
              </p>
              <p>I am a PhD student in the <a href="https://alr.iar.kit.edu/">Autonomous Learning Robots</a> (ALR) at the Karlsruhe Institute of Technology (KIT), Germany.
                My research focuses on robotics and machine learning supervised by <a href="https://alr.iar.kit.edu/21_65.php">Gerhard Neumann</a> and <a href="http://rudolf.intuitive-robots.net/">Rudolf Lioutikov</a>.
              </p>

              <p style="text-align:center">
                <a href="mailto:jia266163@gmail.com">Email</a> &nbsp/&nbsp
<!--                <a href="data/CV_Moritz_Reuss.pdf">CV</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?user=E7Tja9gAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/xiaogangjia">Github</a> &nbsp/&nbsp
<!--                <a href="https://www.linkedin.com/in/moritzreuss/?locale=en_US">LinkedIn</a>-->
              </p>
            </td>

            <td class="circular-image", style="width:20%;max-width:20%">
            <!-- Replace 'your-image.jpg' with the actual path to your image -->
            <img src="images/white.jpg" alt="Circular Image">
<!--            </td>-->
<!--            <td style="padding:2.5%;width:40%;max-width:40%">-->
<!--              <a href="images/white.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/White.jpg" class="hoverZoomLink"></a>-->
<!--            </td>-->
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My primary research goal is to build intelligent embodied agents that assist people in their everyday lives and
                communicate intuitively.
                One of the key challenges to be solved towards this goal is learning from multimodal, uncurated human demonstrations
                without rewards.
                Therefore, I am working on novel methods that exploit multimodality and learn versatile behaviour.
                Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                  <div style="flex: 0 0 25%; max-width: 25%;">
                    <img src='images/mamba.png' style="width: 100%; max-width: 100%;">
                  </div>
                  <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                    <a href="https://openreview.net/pdf?id=IssXUYvVTg">
                      <papertitle>MaIL: Improving Imitation Learning with Selective State Space Models
                      </papertitle></a>
                    <br>
                      <strong>Xiaogang Jia</strong>,
                      Qian Wang,
                      Atalay Donat,
                      Bowen Xing,
                      <a href="https://alr.iar.kit.edu/21_70.php">Ge Li</a>,
                      <a href="https://www.irl.iar.kit.edu/team_66.php">Hongyi Zhou</a>,
                      <a href="https://alr.anthropomatik.kit.edu/21_69.php">Onur Celik</a>,
                    <a href="https://alr.anthropomatik.kit.edu/21_495.php">Denis Blessing</a>,
                      <a href="http://rudolf.intuitive-robots.net/">Rudolf Lioutikov</a>,
                    <a href="https://alr.anthropomatik.kit.edu/21_65.php">Gerhard Neumann</a> <br>
                    <br>
                      CoRL 2024
                      <br>
                      <a href="https://openreview.net/pdf?id=IssXUYvVTg">OpenReview</a>
                      <p></p>
                      <p>
                        This work presents Mamba Imitation Learning (MaIL), a novel imitation learning (IL) architecture that provides an alternative to state-of-the-art (SoTA) Transformer-based policies. MaIL leverages Mamba, a state-space model designed to selectively focus on key features of the data.
                          While Transformers are highly effective in data-rich environments due to their dense attention mechanisms, they can struggle with smaller datasets, often leading to overfitting or suboptimal representation learning. In contrast, Mamba's architecture enhances representation learning efficiency by focusing on key features and reducing model complexity. This approach mitigates overfitting and enhances generalization, even when working with limited data.
                          Extensive evaluations on the LIBERO IL benchmark demonstrate that MaIL consistently outperforms Transformers on all LIBERO tasks with limited data and matches their performance when the full dataset is available. Additionally, MaIL's effectiveness is validated through its superior performance in three real robot experiments.
                      </p>
                  </div>
                </div>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                  <div style="flex: 0 0 25%; max-width: 25%;">
                    <img src='images/sampling.png' style="width: 100%; max-width: 100%;">
                  </div>
                  <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                    <a href="https://openreview.net/pdf?id=fVg9YrSllr">
                      <papertitle>Beyond ELBOs: A Large-Scale Evaluation of Variational Methods for Sampling
                      </papertitle></a>
                    <br>
                    <a href="https://alr.anthropomatik.kit.edu/21_495.php">Denis Blessing</a>,
                    <strong>Xiaogang Jia</strong>,
                     Johannes Esslinger,
                    Francisco Vargas,
                    <a href="https://alr.anthropomatik.kit.edu/21_65.php">Gerhard Neumann</a> <br>
                    <br>
                      ICML 2024
                      <br>
                      <a href="https://openreview.net/pdf?id=fVg9YrSllr">OpenReview</a>
                      <p></p>
                      <p>
                        Monte Carlo methods, Variational Inference, and their combinations play a pivotal role in sampling from intractable probability distributions.
                          However, current studies lack a unified evaluation framework, relying on disparate performance measures and limited method comparisons across diverse tasks,
                          complicating the assessment of progress and hindering the decision-making of practitioners. In response to these challenges,
                          our work introduces a benchmark that evaluates sampling methods using a standardized task suite and a broad range of performance criteria.
                          Moreover, we study existing metrics for quantifying mode collapse and introduce novel metrics for this purpose.
                          Our findings provide insights into strengths and weaknesses of existing sampling methods, serving as a valuable reference for future developments.
                      </p>
                  </div>
                </div>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                  <div style="flex: 0 0 25%; max-width: 25%;">
                    <img src='images/d3il_compressed.gif' style="width: 100%; max-width: 100%;">
                  </div>
                  <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                    <a href="https://openreview.net/pdf?id=6pPYRXKPpw">
                      <papertitle>Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations
                      </papertitle></a>
                    <br>
                    <strong>Xiaogang Jia</strong>,
                    <a href="https://alr.anthropomatik.kit.edu/21_495.php">Denis Blessing</a>,
                    <a href="https://alr.iar.kit.edu/21_500.php">Xinkai Jiang</a>,
                    <a href="https://mbreuss.github.io/">Moritz Reuss</a>,
                    Atalay Donat,
                    <a href="http://rudolf.intuitive-robots.net/">Rudolf Lioutikov</a> <br>,
                    <a href="https://alr.anthropomatik.kit.edu/21_65.php">Gerhard Neumann</a> <br>
                    <br>
                      ICLR 2024
                      <br>
                      <a href="https://openreview.net/pdf?id=6pPYRXKPpw">OpenReview</a>
                      <p></p>
                      <p>
                        Introducing D3IL, a novel set of simulation benchmark environments and datasets tailored for Imitation Learning,
                        D3IL is uniquely designed to challenge and evaluate AI models on their ability to learn and replicate diverse,
                        multi-modal human behaviors. Our environments encompass multiple sub-tasks and object manipulations, providing a rich
                        diversity in behavioral data, a feature often lacking in other datasets. We also introduce practical metrics to
                        effectively quantify a model's capacity to capture and reproduce this diversity. Extensive evaluations of state-of-the-art methods on D3IL offer insightful
                        benchmarks, guiding the development of future imitation learning algorithms capable of generalizing complex human
                        behaviors.
                      </p>
                  </div>
                </div>
              </td>
            </tr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
<!--              <tr style="background-color: #ffffd0;">-->
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                    <div style="display: flex;">
                      <div style="flex: 0 0 25%; max-width: 25%;">
                        <img src='images/beso_kitchen.gif' style="width: 100%; max-width: 100%;">
                      </div>
                        <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                            <a href="https://intuitive-robots.github.io/beso-website">
                                <papertitle>Goal Conditioned Imitation Learning using Score-based Diffusion Policies</papertitle>
                            </a>
                            <br>
                            <a href="https://mbreuss.github.io/">Moritz Reuss</a>,
                            <a href="https://irl.anthropomatik.kit.edu/21_67.php">Maximilian Li</a>,
                            <strong>Xiaogang Jia</strong>,
                            <a href="http://rudolf.intuitive-robots.net/">Rudolf Lioutikov</a> <br>
                            <br>
                            <em><span style="color:red; font-weight:bold;">Best Paper Award</span> @ Workshop on Learning from Diverse, Offline Data
                              (L-DOD) @ ICRA 2023, Robotics: Science and Systems (RSS)</em>, 2023

                            <br>
                            <a href="https://intuitive-robots.github.io/beso-website">project page</a> 
                            /
                            <a href="https://github.com/intuitive-robots/beso">Code </a>
                            /
                            <a href="https://arxiv.org/pdf/2304.02532">arXiv</a>
                            <p></p>
                            <p>
                            We present a novel policy representation, called BESO, for goal-conditioned imitation learning using score-based diffusion models.
                            BESO is able to effectively learn goal-directed, multi-modal behavior from uncurated reward-free offline-data.
                            On several challening benchmarks our method outperforms current policy representation by a wide margin. 
                            BESO can also be used as a standard policy for imitation learning and achieves state-of-the-art performance
                            with only 3 denoising steps. 
                            </p>
                        </div>
                    </div>
                </td>
            </tr>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                  <div style="flex: 0 0 25%; max-width: 25%;">
                    <img src='images/IMC_obstacle_avoidance.png' style="width: 100%; max-width: 100%;">
                  </div>
                  <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                    <a href="https://arxiv.org/pdf/2303.15349">
                      <papertitle>Information Maximizing Curriculum: A Curriculum-Based Approach for Learning Versatile Skills
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://alr.anthropomatik.kit.edu/21_495.php">Denis Blessing</a>,
                    <a href="https://alr.anthropomatik.kit.edu/21_69.php">Onur Celik</a>,
                    <strong>Xiaogang Jia</strong>,
                    <a href="https://mbreuss.github.io/">Moritz Reuss</a>,
                    <a href="https://irl.anthropomatik.kit.edu/21_67.php">Maximilian Xiling</a>,
                    <a href="http://rudolf.intuitive-robots.net/">Rudolf Lioutikov</a> <br>,
                    <a href="https://alr.anthropomatik.kit.edu/21_65.php">Gerhard Neumann</a> <br>
                    <br>
                    <em>Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) </em>, 2023
                    <br>
                    <a href="https://arxiv.org/pdf/2303.15349">arXiv</a>
                    <p></p>
                    <p>
                      We introduce the Information Maximizing Curriculum method to address mode-averaging in imitation learning by enabling
                      the model to specialize in representable data. This approach is enhanced by a mixture of experts (MoE) policy, each
                      focusing on different data subsets, and employs a unique maximum entropy-based objective for full dataset coverage.
                      </p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                  <div style="flex: 0 0 25%; max-width: 25%;">
                    <img src='images/gaoxing.png' style="width: 100%; max-width: 100%;">
                  </div>
                  <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081482">
                      <papertitle>Dynamic Scenario Representation Learning for Motion Forecasting with Heterogeneous Graph Convolutional Recurrent Networks
                      </papertitle>
                    </a>
                    <br>
                    Xing Gao,
                    <strong>Xiaogang Jia</strong>,
                    Yikang Li,
                      Hongkai Xiong
                    <br>
                    <em>IEEE Robotics and Automation Letters</em>, 2023
                    <br>
                    <a href="https://arxiv.org/abs/2303.04364">arXiv</a>
                    <p></p>
                    <p>In this paper, we resort to dynamic heterogeneous graphs to model the scenario. Various scenario components including vehicles (agents) and lanes, multi-type interactions,
                        and their changes over time are jointly encoded. Furthermore, we design a novel heterogeneous graph convolutional recurrent network, aggregating diverse interaction information and
                        capturing their evolution, to learn to exploit intrinsic spatio-temporal dependencies in dynamic graphs and obtain effective representations of dynamic scenarios. Finally,
                        with a motion forecasting decoder, our model predicts realistic and multi-modal future trajectories of agents and outperforms state-of-the-art published works on several motion forecasting benchmarks.
                      </p>
              </td>
            </tr>

<tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                  <div style="flex: 0 0 25%; max-width: 25%;">
                    <img src='images/yeping.png' style="width: 100%; max-width: 100%;">
                  </div>
                  <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812188">
                      <papertitle>Causal-based Time Series Domain Generalization for Vehicle Intention Prediction
                      </papertitle>
                    </a>
                    <br>
                    Yeping Hu,
                    <strong>Xiaogang Jia</strong>,
                    Masayoshi Tomizuka,
                      Wei Zhan
                    <br>
                    <em>International Conference on Robotics and Automation (ICRA)</em>, 2022
                    <br>
                    <a href="https://arxiv.org/abs/2112.02093">arXiv</a>
                    <p></p>
                    <p> We construct a structural causal model for vehicle intention
                        prediction tasks to learn an invariant representation of input
                        driving data for domain generalization. We further integrate a
                        recurrent latent variable model into our structural causal model
                        to better capture temporal latent dependencies from time-series
                        input data. The effectiveness of our approach is evaluated via
                        real-world driving data.
                      </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                  <div style="flex: 0 0 25%; max-width: 25%;">
                    <img src='images/liting.png' style="width: 100%; max-width: 100%;">
                  </div>
                  <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                    <a href="https://www.roboticsproceedings.org/rss17/p037.pdf">
                      <papertitle>On complementing end-to-end human motion predictors with planning
                      </papertitle>
                    </a>
                    <br>
                    Liting Sun,
                    <strong>Xiaogang Jia</strong>,
                    Anca D Dragan
                    <br>
                    <em>Robotics: Science and Systems (RSS)</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2103.05661">arXiv</a>
                    <p></p>
                    <p>In this work, we analyze one family of approaches that strive to get the best of both worlds: use the end-to-end predictor on common cases,
                        but do not rely on it for tail events / out-of-distribution inputs -- switch to the planning-based predictor there. We contribute an analysis of
                        different approaches for detecting when to make this switch, using an autonomous driving domain. We find that promising approaches based on ensembling or generative
                        modeling of the training distribution might not be reliable,
                        but that there very simple methods which can perform surprisingly well -- including training a classifier to pick up on tell-tale issues in predicted trajectories.
                      </p>
              </td>
            </tr>

            <tr style="background-color: #ffffd0;">
            </tr>


</tbody>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
      <td style="padding:0px; vertical-align: middle;">
          <br>
          <p style="text-align:right;font-size:small;">
              The website is based on the code from <a href="https://github.com/jonbarron/jonbarron_website">source code</a>!
          </p>
      </td>
  </tr>
</tbody></table>
</body>

</html>
